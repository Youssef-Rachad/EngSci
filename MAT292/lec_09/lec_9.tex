\documentclass[12pt]{article}
\author{niceguy}
\title{Lecture 9}
\usepackage{../../template}
\begin{document}
\maketitle

\section{Nonhomogeneous to Homogeneous solutions}
Consider a first order lienar system with constant coefficients
$$\vec{x}' = A\vec{x} + \vec{b}$$
where
$$\vec{x}(t) = \begin{pmatrix} x_1(t) \\ x_2(t) \end{pmatrix}, A = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix}, \vec{b} = \begin{pmatrix} b_1 \\ b_2 \end{pmatrix}$$
Consider an ansatz in the form
$$\vec{x} = \vec{\tilde{x}} + \vec{x_{eq}}$$
where $\vec{x_{eq}}$ is independent of $t$.
Plugging this into the ODE,
$$\vec{\tilde{x}}' = A\vec{\tilde{x}} + A\vec{x_{eq}} + \vec{b}$$
Assuming $A$ is invertible, if we define
$$\vec{x_{eq}} = -A^{-1}\vec{b}$$
we have a homogeneous linear system
$$\vec{\tilde{x}} = A\vec{\tilde{x}}$$

\section{Superposition Principle}
Let $\phi_1(t)$ and $\phi_2(t)$ be solutions to a first order linear system
$$\vec{x}' = A\vec{x}$$
Then obviously (differentiation and matrix multiplication are linear functions) any linear combination
$$c_1\phi_1(t) + c_2\phi_2(t)$$
is also a solution.
\begin{thm}
	Suppose $\vec{\phi_1}(t)$ and $\vec{\phi_2}(t)$ are solutions of $\vec{x}' = A\vec{x}$. Then for any coefficients $c_1$ and $c_2$,
	$$c_1\vec{\phi_1}(t) + c_2\vec{\phi_2}(t)$$
	is also a solution.
\end{thm}
\begin{defn}
	Suppose we have two solutions $\vec{\phi_1}(t)$ and $\vec{\phi_2}(t)$ for the system $\vec{x}' = A\vec{x}$ defined on $I$. We say they are linearly independent if $\exists k \in \R$ such that
	$$\vec{\phi_1}(t) = k\vec{\phi_2}(t)$$
	Otherwise we say they are linearly independent.
\end{defn}

Let $\vec{x}(0) = \vec{x_0}$. We can then solve for the constants $c_1$ and $c_2$ by

$$\begin{pmatrix} \phi_1^1(t_0) & \phi_2^1(t_0) \\ \phi_2^1(t_0) & \phi_2^2(t_0)\end{pmatrix} \begin{pmatrix} c_1 \\ c_2 \end{pmatrix} = \begin{pmatrix} x_0^1 \\ x_0^2 \end{pmatrix}$$
A unique solution is guaranteed as long as the $2\times2$ matrix is invertible.

\section{Wronskian}

\begin{defn}
	$$W[\vec{\phi_1}, \vec{\phi_2}](t) = \text{det}\begin{bmatrix} \phi_1^1(t) & \phi_2^1(t) \\ \phi_1^2(t) & \phi_2^2(t) \end{bmatrix}$$
\end{defn}

\begin{thm}
	Suppose $\vec{\phi_1}(t)$ and $\vec{\phi_2}(t)$ are two solutions to a homogeneous first order linear system
	$$\vec{x}' = A\vec{x}$$
	for an interval $I$. If the Wronskian is non zero $\forall t \in I$, the general solution is given by
	$$c_1\vec{\phi_1}(t) + c_2\vec{\phi_2}(t), c_1,c_2 \in \R$$
\end{thm}

\begin{thm}
	Suppose $\vec{\phi_1}(t)$ and $\vec{\phi_2}(t)$ are two solutions to a homogeneous first order linear system
	$$\vec{x}' = A\vec{x}$$
	They are linearly independent if and only if the Wronskian is non zero.
\end{thm}

Note that it is impossible for the Wronskian to be 0 at only a single point (proof will come later). \\
From the above, we know all we need is two linearly independent solutions. Consider the ansatz
$$\vec{x} = e^{\lambda t}\vec{v}$$
where $\vec{v}$ is independent of $t$. Substitution gives us
$$\lambda \vec{v} = A\vec{v}$$
so $\vec{v}$ is an eigenvector, and $\lambda$ is an eigenvalue. \\
Now there are several cases. We first consider
$$\lambda_1 \neq \lambda_2, \lambda_1, \lambda_2 \in \R$$
Then we have $$\vec{\phi_1}(t) = e^{\lambda_1t}\vec{v_1}, \vec{\phi_2}(t) = e^{\lambda_2t}\vec{v_2}$$
They are independent, as the two eigenvectors must be linearly independent. \\
\end{document}
